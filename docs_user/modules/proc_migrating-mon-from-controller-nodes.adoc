[id="migrating-mon-from-controller-nodes_{context}"]

= Migrating Ceph Monitor daemons to {Ceph} nodes

The following section describes how to move Ceph Monitor daemons from the
{rhos_prev_long} Controller nodes to a set of target nodes. Target nodes might
be pre-existing {Ceph} nodes, or {OpenStackShort} Compute nodes if {Ceph} is
deployed by {OpenStackPreviousInstaller} with an HCI topology.
This procedure assumes that some of the steps are run on the source node that
we want to decommission, while other steps are run on the target node that is
supposed to host the redeployed daemon.


.Prerequisites

Configure the target nodes (CephStorage or ComputeHCI) to have both `storage`
and `storage_mgmt` networks to ensure that you can use both {Ceph} public and
cluster networks from the same node. This step requires you to interact with
{OpenStackPreviousInstaller}. From {rhos_prev_long} {rhos_prev_ver} and later
you do not have to run a stack update. However, there are commands that you
must perform to run `os-net-config` on the bare metal node and configure
additional networks.

. If target nodes are `CephStorage`, ensure that the network is defined in the
`metalsmith.yaml` for the CephStorageNodes:
+
[source,yaml]
----
  - name: CephStorage
    count: 2
    instances:
      - hostname: oc0-ceph-0
        name: oc0-ceph-0
      - hostname: oc0-ceph-1
        name: oc0-ceph-1
    defaults:
      networks:
        - network: ctlplane
          vif: true
        - network: storage_cloud_0
            subnet: storage_cloud_0_subnet
        - network: storage_mgmt_cloud_0
            subnet: storage_mgmt_cloud_0_subnet
      network_config:
        template: templates/single_nic_vlans/single_nic_vlans_storage.j2
----

. Run the provisioning command:
+
----
$ openstack overcloud node provision \
  -o overcloud-baremetal-deployed-0.yaml --stack overcloud-0 \
  --network-config -y --concurrency 2 /home/stack/metalsmith-0.yam
----

. Verify that the storage network is configured on the target nodes:
+
----
(undercloud) [stack@undercloud ~]$ ssh heat-admin@192.168.24.14 ip -o -4 a
1: lo    inet 127.0.0.1/8 scope host lo\       valid_lft forever preferred_lft forever
5: br-storage    inet 192.168.24.14/24 brd 192.168.24.255 scope global br-storage\       valid_lft forever preferred_lft forever
6: vlan1    inet 192.168.24.14/24 brd 192.168.24.255 scope global vlan1\       valid_lft forever preferred_lft forever
7: vlan11    inet 172.16.11.172/24 brd 172.16.11.255 scope global vlan11\       valid_lft forever preferred_lft forever
8: vlan12    inet 172.16.12.46/24 brd 172.16.12.255 scope global vlan12\       valid_lft forever preferred_lft forever
----

.Procedure

. Ssh into the target node and enable the firewall rules that are required to
  reach a Mon service:
+
----
$ for port in 3300 6789; {
    ssh heat-admin@<target_node> sudo iptables -I INPUT \
    -p tcp -m tcp --dport $port -m conntrack --ctstate NEW \
    -j ACCEPT;
}
----
+
* Replace `<target_node>` with the hostname of the node that is supposed to
  host the new mon.

. Check that the rules are properly applied and persist them:
+
----
sudo iptables-save
sudo systemctl restart iptables
----

. To migrate the existing Mons to the target {Ceph} nodes, create the following
  {Ceph} spec from the first mon (or the first Controller node) and modify the
  placement based on the appropriate label.
+
[source,yaml]
----
service_type: mon
service_id: mon
placement:
  label: mon
----

. Save the spec in the `/tmp/mon.yaml` file.
. Apply the spec with cephadm by using the orchestrator:
+
----
$ sudo cephadm shell -m /tmp/mon.yaml
$ ceph orch apply -i /mnt/mon.yaml
----

. Extend the `mon` label to the rest of the {Ceph} target nodes to ensure that
  you never lose quorum during the migration process:
+
----
declare -A target_nodes

target_nodes[mon]="oc0-ceph-0 oc0-ceph-1 oc0-ceph2"

mon_nodes="${target_nodes[mon]}"
IFS=' ' read -r -a mons <<< "$mon_nodes"

for node in "${mons[@]}"; do
    ceph orch host add label $node mon
    ceph orch host add label $node _admin
done
----
+
[NOTE]
Applying the `mon.yaml` spec allows the existing strategy to use `labels`
instead of `hosts`. As a result, any node with the `mon` label can host a Ceph
mon daemon.
Execute this step once to avoid multiple iterations when multiple Ceph Mons are
migrated.

. Check the status of the {CephCluster} and the Ceph orchestrator daemons list.
  Make sure that mons are in quorum and listed by the `ceph orch`
  command:
+
----
# ceph -s
  cluster:
    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3
    health: HEALTH_OK

  services:
    mon: 6 daemons, quorum oc0-controller-0,oc0-controller-1,oc0-controller-2,oc0-ceph-0,oc0-ceph-1,oc0-ceph-2 (age 19m)
    mgr: oc0-controller-0.xzgtvo(active, since 32m), standbys: oc0-controller-1.mtxohd, oc0-controller-2.ahrgsk
    osd: 8 osds: 8 up (since 12m), 8 in (since 18m); 1 remapped pgs

  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   43 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
----
+
----
[ceph: root@oc0-controller-0 /]# ceph orch host ls
HOST              ADDR           LABELS          STATUS
oc0-ceph-0        192.168.24.14  osd mon _admin
oc0-ceph-1        192.168.24.7   osd mon _admin
oc0-ceph-2        192.168.24.8   osd mon _admin
oc0-controller-0  192.168.24.15  _admin mgr mon
oc0-controller-1  192.168.24.23  _admin mgr mon
oc0-controller-2  192.168.24.13  _admin mgr mon
----

. On the source node, back up the `/etc/ceph/` directory. The backup allows you
  to execute cephadm and get a shell to the {Ceph} cluster from the source node:
+
----
$ mkdir -p $HOME/ceph_client_backup
$ sudo cp -R /etc/ceph $HOME/ceph_client_backup
----

. Before draining the source node and relocating the IP address of the storage
  network to the target node, fail the ceph-mgr if it is active on the
  source node:
+
----
$ ceph mgr fail <mgr instance>
----

. Drain the source node and start the mon migration. From the cephadm shell,
  remove the labels on the source node:
+
----
for label in mon mgr _admin; do
    ceph orch host rm label <source_node> $label;
done
----

. Remove the running mon daemon from the source node:
+
----
$ cephadm shell -- ceph orch daemon rm mon.<source_node> --force"
----

. Run the drain command:
+
----
$ cephadm shell -- ceph drain <source_node>
----

. Remove the `<source_node>` host from the {CephCluster} cluster:
+
----
$ cephadm shell -- ceph orch host rm <source_node> --force"
----
+
* Replace `<source_node>` with the hostname of the source node.
+

[NOTE]
The source node is not part of the cluster anymore, and should not appear in
the {Ceph} host list when `cephadm shell -- ceph orch host ls` is run.
However, a `sudo podman ps` in the `<source_node>` might list both mon and mgr
still up and running.

----
[root@oc0-controller-1 ~]# sudo podman ps
CONTAINER ID  IMAGE                                                                                        COMMAND               CREATED         STATUS             PORTS       NAMES
ifeval::["{build}" != "downstream"]
5c1ad36472bc  quay.io/ceph/daemon@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mon.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mon-oc0-controller-1
3b14cc7bf4dd  quay.io/ceph/daemon@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mgr.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mgr-oc0-controller-1-mtxohd
endif::[]
ifeval::["{build}" == "downstream"]
5c1ad36472bc  registry.redhat.io/ceph/rhceph@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mon.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mon-oc0-controller-1
3b14cc7bf4dd  registry.redhat.io/ceph/rhceph@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mgr.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mgr-oc0-controller-1-mtxohd
endif::[]
----

To cleanup the source node before moving to the next phase, cleanup the existing
containers and remove the cephadm related data from the node.
// fpantano: there's an automated procedure run through cephadm but it's too
// risky. If the user doesn't perform it properly the cluster can be affected.
// We can put a downstream comment to contact the RH support to clean the source
// node up in case of leftovers, and open a bug for cephadm.

//. ssh into one of the existing Ceph mons (usually controller-1 or controller-2)
. Prepare the target node to host the new mon and add the `mon` label to the
target node:
+
----
for label in mon mgr _admin; do
    ceph orch host label add <target_node> $label; done
done
----
+
* Replace <target_node> with the hostname of the host listed in the {CephCluster}
  through the `ceph orch host ls` command.

. Confirm that mons are in quorum:
+
----
$ cephadm shell -- ceph -s
$ cephadm shell -- ceph orch ps | grep -i mon
----

It is now possible to migrate the original mon IP address to the target node and
redeploy the existing mon on it.
The following IP address migration procedure assumes that the target nodes have
been originally deployed by {OpenStackPreviousInstaller} and the network configuration
is managed by `os-net-config`.

// NOTE (fpantano): we need to document the same ip address migration procedure
// w/ an EDPM node that has already been adopted.

. Get the mon ip address from the existing `/etc/ceph/ceph.conf` (check the `mon_host`
line), for example:
+
----
mon_host = [v2:172.17.3.60:3300/0,v1:172.17.3.60:6789/0] [v2:172.17.3.29:3300/0,v1:172.17.3.29:6789/0] [v2:172.17.3.53:3300/0,v1:172.17.3.53:6789/0]
----

. Confirm that the mon ip address is present on the source node `os-net-config`
configuration located in `/etc/os-net-config`:
+
----

[tripleo-admin@controller-0 ~]$ grep "172.17.3.60" /etc/os-net-config/config.yaml
    - ip_netmask: 172.17.3.60/24
----

. Edit `/etc/os-net-config/config.yaml` and remove the `ip_netmask` line.

. Save the file and refresh the node network configuration:
+
----
$ sudo os-net-config -c /etc/os-net-config/config.yaml
----

. Verify, using the `ip` command, that the IP address is not present in the source
node anymore.

. Ssh into the target node, for example `cephstorage-0`, and add the IP address
  for the new mon.

. On the target node, edit `/etc/os-net-config/config.yaml` and
add the `- ip_netmask: 172.17.3.60` line that you removed in the source node.

. Save the file and refresh the node network configuration:
+
----
$ sudo os-net-config -c /etc/os-net-config/config.yaml
----

. Verify, using the `ip` command, that the IP address is present in the target
node.

. Get the ceph mon spec:
+
----
ceph orch ls --export mon > mon.yaml
----

. Edit the retrieved spec and add the `unmanaged: true` keyword:
+
[source,yaml]
----
service_type: mon
service_id: mon
placement:
  label: mon
unmanaged: true
----

. Save the spec in the `/tmp/mon.yaml` file
. Apply the spec with cephadm by using the orchestrator:
+
----
$ sudo cephadm shell -m /tmp/mon.yaml
$ ceph orch apply -i /mnt/mon.yaml
----
+
The mon daemons are marked as `<unmanaged>`, and it is now possible to redeploy
the existing daemon and bind it to the migrated IP address.

. Delete the existing mon on the target node:
+
----
$ ceph orch daemon add rm mon.<target_node> --force
----
+
. Redeploy the new mon on the target using the old IP address:
+
----
$ ceph orch daemon add mon <target_node>:<ip_address>
----
+
* Replace `<target_node>` with the hostname of the target node enrolled in the
  {Ceph} cluster.
* Replace `<ip_address>` with the ip address of the migrated address.


. Get the ceph mon spec:
+
----
$ ceph orch ls --export mon > mon.yaml
----

. Edit the retrieved spec and set the `unmanaged` keyword to `false`:
+
[source,yaml]
----
service_type: mon
service_id: mon
placement:
  label: mon
unmanaged: false
----

. Save the spec in `/tmp/mon.yaml` file.
. Apply the spec with cephadm by using the Ceph Orchestrator:
+
----
$ sudo cephadm shell -m /tmp/mon.yaml
$ ceph orch apply -i /mnt/mon.yaml
----
+
The new mon runs on the target node with the original IP address.

. Identify the running `mgr`:
+
----
$ sudo cephadm shell -- ceph -s
----
+
. Refresh the mgr information by force-failing it:
+
----
$ ceph mgr fail
----
+
. Refresh the `OSD` information:
+
----
$ ceph orch reconfig osd.default_drive_group
----
+
Verify the {CephCluster} cluster is healthy:
+
----
[ceph: root@oc0-controller-0 specs]# ceph -s
  cluster:
    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3
    health: HEALTH_OK
...
...
----

. Repeat this procedure for any additional Controller node that hosts a mon
  until you have migrated all the Ceph Mon daemons to the target nodes.
