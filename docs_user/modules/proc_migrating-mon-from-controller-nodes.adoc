[id="migrating-mon-from-controller-nodes_{context}"]

= Migrating Ceph Monitor daemons to {Ceph} nodes

You must move Ceph Monitor daemons from the {rhos_prev_long} ({OpenStackShort}) Controller nodes to a set of target nodes. Target nodes are either existing {Ceph} nodes, or {OpenStackShort} Compute nodes if {Ceph} is
deployed by {OpenStackPreviousInstaller} with a Hyperconverged Infrastructure (HCI) topology.

To migrate the Ceph Monitor daemons, the following actions occur:

* The mon IP addresses are moved to the target {Ceph} nodes. The IP address migration assumes that the target nodes are originally deployed by {OpenStackPreviousInstaller} and that the network configuration is managed by `os-net-config`.
* The existing Controller nodes are drained and decommisioned.
* Additional monitors are deployed to the target nodes, and they are promoted
as `_admin` nodes that you can use to manage the {CephCluster} cluster and
perform day 2 operations.

.Prerequisites

* The target nodes, CephStorage or ComputeHCI, are configured to have both `storage` and `storage_mgmt` networks. This ensures that you can use both {Ceph} public and cluster networks from the same node. This step requires you to interact with {OpenStackPreviousInstaller}. From {OpenStackShort} {rhos_prev_ver} and later you do not have to run a stack update.
+
[NOTE]
This step requires you to interact with {OpenStackPreviousInstaller}. From {OpenStackShort} {rhos_prev_ver} and later you do not have to run a stack update.
* Run `os-net-config` on the bare metal node and configure additional networks:
.. If target nodes are `CephStorage`, ensure that the network is defined in the
`metalsmith.yaml` for the `CephStorage` nodes:
+
[source,yaml]
----
  - name: CephStorage
    count: 2
    instances:
      - hostname: oc0-ceph-0
        name: oc0-ceph-0
      - hostname: oc0-ceph-1
        name: oc0-ceph-1
    defaults:
      networks:
        - network: ctlplane
          vif: true
        - network: storage_cloud_0
            subnet: storage_cloud_0_subnet
        - network: storage_mgmt_cloud_0
            subnet: storage_mgmt_cloud_0_subnet
      network_config:
        template: templates/single_nic_vlans/single_nic_vlans_storage.j2
----

.. Provision the `CephStorage` nodes:
+
----
$ openstack overcloud node provision \
  -o overcloud-baremetal-deployed-0.yaml --stack overcloud-0 \
  --network-config -y --concurrency 2 /home/stack/metalsmith-0.yam
----

.. Verify that the storage network is configured on the target nodes:
+
----
(undercloud) [stack@undercloud ~]$ ssh heat-admin@192.168.24.14 ip -o -4 a
1: lo    inet 127.0.0.1/8 scope host lo\       valid_lft forever preferred_lft forever
5: br-storage    inet 192.168.24.14/24 brd 192.168.24.255 scope global br-storage\       valid_lft forever preferred_lft forever
6: vlan1    inet 192.168.24.14/24 brd 192.168.24.255 scope global vlan1\       valid_lft forever preferred_lft forever
7: vlan11    inet 172.16.11.172/24 brd 172.16.11.255 scope global vlan11\       valid_lft forever preferred_lft forever
8: vlan12    inet 172.16.12.46/24 brd 172.16.12.255 scope global vlan12\       valid_lft forever preferred_lft forever
----

.Procedure

. SSH into the target node and enable the firewall rules that are required to
reach a Ceph Monitor service:
+
----
$ for port in 3300 6789; {
    ssh heat-admin@<target_node> sudo iptables -I INPUT \
    -p tcp -m tcp --dport $port -m conntrack --ctstate NEW \
    -j ACCEPT;
}
----
+
* Replace `<target_node>` with the hostname of the node that hosts the new Ceph Monitor.

. Check that the rules are properly applied to the target node and persist them:
+
----
$ sudo iptables-save
$ sudo systemctl restart iptables
----

. To migrate the existing Ceph Monitors to the target {Ceph} nodes, create the following {Ceph} spec from the first Ceph Monitor, or the first Controller node, and add the `label:mon` section to the `placement` section:
+
[source,yaml]
----
service_type: mon
service_id: mon
placement:
  label: mon
----

. Save the spec in the `/tmp/mon.yaml` file.

. Apply the spec with `cephadm` by using the Ceph Orchestrator:
+
----
$ sudo cephadm shell -m /tmp/mon.yaml
$ ceph orch apply -i /mnt/mon.yaml
----

. Extend the `mon` label to the rest of the {Ceph} target nodes to ensure that
  you never lose quorum during the migration process:
+
----
declare -A target_nodes

target_nodes[mon]="oc0-ceph-0 oc0-ceph-1 oc0-ceph2"

mon_nodes="${target_nodes[mon]}"
IFS=' ' read -r -a mons <<< "$mon_nodes"

for node in "${mons[@]}"; do
    ceph orch host add label $node mon
    ceph orch host add label $node _admin
done
----
+
[NOTE]
Applying the `mon.yaml` spec allows the existing strategy to use `labels`
instead of `hosts`. As a result, any node with the `mon` label can host a Ceph
Monitor daemon. Perform this step only once to avoid multiple iterations when multiple Ceph Monitors are migrated.

. Check the status of the {CephCluster} and the Ceph orchestrator daemons list.
  Make sure that mons are in quorum and listed by the `ceph orch`
  command:
+
----
# ceph -s
  cluster:
    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3
    health: HEALTH_OK

  services:
    mon: 6 daemons, quorum oc0-controller-0,oc0-controller-1,oc0-controller-2,oc0-ceph-0,oc0-ceph-1,oc0-ceph-2 (age 19m)
    mgr: oc0-controller-0.xzgtvo(active, since 32m), standbys: oc0-controller-1.mtxohd, oc0-controller-2.ahrgsk
    osd: 8 osds: 8 up (since 12m), 8 in (since 18m); 1 remapped pgs

  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   43 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
----
+
----
[ceph: root@oc0-controller-0 /]# ceph orch host ls
HOST              ADDR           LABELS          STATUS
oc0-ceph-0        192.168.24.14  osd mon _admin
oc0-ceph-1        192.168.24.7   osd mon _admin
oc0-ceph-2        192.168.24.8   osd mon _admin
oc0-controller-0  192.168.24.15  _admin mgr mon
oc0-controller-1  192.168.24.23  _admin mgr mon
oc0-controller-2  192.168.24.13  _admin mgr mon
----

. On the source node, back up the `/etc/ceph/` directory to run `cephadm` and get a shell for the {Ceph} cluster from the source node:
+
----
$ mkdir -p $HOME/ceph_client_backup
$ sudo cp -R /etc/ceph $HOME/ceph_client_backup
----

. Before you drain the source node and relocate the IP address of the storage
network to the target node, fail the `ceph-mgr` if it is active on the
source node:
+
----
$ ceph mgr fail <mgr_instance>
----
+
* Replace `<mgr_instance>` with the Ceph Manager daemon to fail.

. Drain the source node and start the Ceph Monitor migration. From the `cephadm` shell, remove the labels on the source node:
+
----
for label in mon mgr _admin; do
    ceph orch host rm label <source_node> $label;
done
----
+
* Replace `<source_node>` with the hostname of the source node.

. Remove the running Ceph Monitor daemon from the source node:
+
----
$ cephadm shell -- ceph orch daemon rm mon.<source_node> --force"
----

. Drain the source node:
+
----
$ cephadm shell -- ceph drain <source_node>
----

. Remove the source node host from the {CephCluster} cluster:
+
----
$ cephadm shell -- ceph orch host rm <source_node> --force"
----
+
[NOTE]
====
The source node is not part of the cluster anymore, and should not appear in
the {Ceph} host list when `cephadm shell -- ceph orch host ls` is run.
However, if you run `sudo podman ps` in the source node, the list might show that both Ceph Monitors and Ceph Managers are still up and running.

----
[root@oc0-controller-1 ~]# sudo podman ps
CONTAINER ID  IMAGE                                                                                        COMMAND               CREATED         STATUS             PORTS       NAMES
ifeval::["{build}" != "downstream"]
5c1ad36472bc  quay.io/ceph/daemon@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mon.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mon-oc0-controller-1
3b14cc7bf4dd  quay.io/ceph/daemon@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mgr.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mgr-oc0-controller-1-mtxohd
endif::[]
ifeval::["{build}" == "downstream"]
5c1ad36472bc  registry.redhat.io/ceph/rhceph@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mon.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mon-oc0-controller-1
3b14cc7bf4dd  registry.redhat.io/ceph/rhceph@sha256:320c364dcc8fc8120e2a42f54eb39ecdba12401a2546763b7bef15b02ce93bc4  -n mgr.oc0-contro...  35 minutes ago  Up 35 minutes ago              ceph-f6ec3ebe-26f7-56c8-985d-eb974e8e08e3-mgr-oc0-controller-1-mtxohd
endif::[]
----
ifeval::["{build}" == "downstream"]
To clean up the existing containers and remove the `cephadm` data from the source node, contact Red Hat Support.
endif::[]
====
// fpantano: there's an automated procedure run through cephadm but it's too
// risky. If the user doesn't perform it properly the cluster can be affected.
// We can put a downstream comment to contact the RH support to clean the source
// node up in case of leftovers, and open a bug for cephadm.
//. ssh into one of the existing Ceph mons (usually controller-1 or controller-2)

. Prepare the target node to host the new Ceph Monitor and add the `mon` label to the target node:
+
----
for label in mon mgr _admin; do
    ceph orch host label add <target_node> $label; done
done
----
+
* Replace `<target_node>` with the hostname of the host listed in the {CephCluster} through the `ceph orch host ls` command.

. Confirm that mons are in quorum:
+
----
$ cephadm shell -- ceph -s
$ cephadm shell -- ceph orch ps | grep -i mon
----
+
// NOTE (fpantano): we need to document the same ip address migration procedure
// w/ an EDPM node that has already been adopted.

. Get the original Ceph Monitor IP address from the existing `/etc/ceph/ceph.conf` file on the `mon_host` line, for example:
+
----
mon_host = [v2:172.17.3.60:3300/0,v1:172.17.3.60:6789/0] [v2:172.17.3.29:3300/0,v1:172.17.3.29:6789/0] [v2:172.17.3.53:3300/0,v1:172.17.3.53:6789/0]
----

. Confirm that the Ceph Monitor IP address is present in the `os-net-config` configuration that is located in the `/etc/os-net-config` directory on the source node:
+
----
[tripleo-admin@controller-0 ~]$ grep "172.17.3.60" /etc/os-net-config/config.yaml
    - ip_netmask: 172.17.3.60/24
----

. Edit the `/etc/os-net-config/config.yaml` file and remove the `ip_netmask` line.

. Save the file and refresh the node network configuration:
+
----
$ sudo os-net-config -c /etc/os-net-config/config.yaml
----

. Verify, using the `ip` command, that the IP address is not present in the source node anymore.

. SSH into the target node, for example `cephstorage-0`, and add the IP address
for the new mon.

. On the target node, edit `/etc/os-net-config/config.yaml` and
add the `- ip_netmask: 172.17.3.60` line that you removed in the source node.

. Save the file and refresh the node network configuration:
+
----
$ sudo os-net-config -c /etc/os-net-config/config.yaml
----

. Verify that the IP address is present in the target node.

. Get the ceph mon spec:
+
----
$ ceph orch ls --export mon > mon.yaml
----

. Edit the retrieved spec and add the `unmanaged: true` keyword:
+
[source,yaml]
----
service_type: mon
service_id: mon
placement:
  label: mon
unmanaged: true
----

. Save the spec in the `/tmp/mon.yaml` file.

. Apply the spec with `cephadm` by using the Ceph Orchestrator:
+
----
$ sudo cephadm shell -m /tmp/mon.yaml
$ ceph orch apply -i /mnt/mon.yaml
----
+
The Ceph Monitor daemons are marked as `unmanaged`, and you can now redeploy the existing daemon and bind it to the migrated IP address.

. Delete the existing Ceph Monitor on the target node:
+
----
$ ceph orch daemon add rm mon.<target_node> --force
----
+
* Replace `<target_node>` with the hostname of the target node that is included in the {Ceph} cluster.

. Redeploy the new Ceph Monitor on the target node by using the previous IP address:
+
----
$ ceph orch daemon add mon <target_node>:<ip_address>
----
+
* Replace `<ip_address>` with the IP address of the migrated IP address.

. Get the Ceph Monitor spec:
+
----
$ ceph orch ls --export mon > mon.yaml
----

. Edit the retrieved spec and set the `unmanaged` keyword to `false`:
+
[source,yaml]
----
service_type: mon
service_id: mon
placement:
  label: mon
unmanaged: false
----

. Save the spec in the `/tmp/mon.yaml` file.

. Apply the spec with `cephadm` by using the Ceph Orchestrator:
+
----
$ sudo cephadm shell -m /tmp/mon.yaml
$ ceph orch apply -i /mnt/mon.yaml
----
+
The new Ceph Monitor runs on the target node with the original IP address.

. Identify the running `mgr`:
+
----
$ sudo cephadm shell -- ceph -s
----
+
. Refresh the Ceph Manager information by force-failing it:
+
----
$ ceph mgr fail
----
+
. Refresh the `OSD` information:
+
----
$ ceph orch reconfig osd.default_drive_group
----

.Verification

* Verify that the {CephCluster} cluster is healthy:
+
----
[ceph: root@oc0-controller-0 specs]# ceph -s
  cluster:
    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3
    health: HEALTH_OK
...
...
----

.Next steps

* Repeat this procedure for any additional Controller node that hosts a Ceph Monitor until you migrate all the Ceph Monitor daemons to the target nodes.
