[id="creating-a-ceph-nfs-cluster_{context}"]

= Creating an NFS Ganesha cluster

If you use the Ceph via NFS back end with the {rhos_component_storage_file_first_ref}, you must create a new clustered NFS service on the {Ceph} cluster. This service replaces the standalone, Pacemaker-controlled `ceph-nfs` service that you use in {rhos_prev_long} ({OpenStackShort}) {rhos_prev_ver}.

.Procedure

. Identify the {Ceph} nodes to deploy the new clustered NFS service. You must deploy this service on the `StorageNFS` isolated network so that
you can mount your existing shares through the new NFS export locations.
. To propagate the `StorageNFS` network to the target nodes where the `ceph-nfs` service is deployed, identify the node definition file that is used in the {OpenStackShort} environment. This is the input file associated with the `openstack overcloud node provision` command. For example, this file might be called `overcloud-baremetal-deploy.yaml`.
ifeval::["{build}" != "downstream"]
See link:https://docs.openstack.org/project-deploy-guide/tripleo-docs/wallaby/features/network_isolation.html#deploying-the-overcloud-with-network-isolation[Deploying
an Overcloud with Network Isolation with TripleO] and link:https://docs.openstack.org/project-deploy-guide/tripleo-docs/wallaby/post_deployment/updating_network_configuration_post_deployment.html[Applying
network configuration changes after deployment] for the background to these
tasks.
endif::[]
The following steps are relevant if you deployed your {Ceph} nodes with {OpenStackPreviousInstaller}.

. Edit the networks that are associated with the {CephCluster} nodes to include the `StorageNFS` network:
+
[source,yaml]
----
- name: CephStorage
  count: 3
  hostname_format: cephstorage-%index%
  instances:
  - hostname: cephstorage-0
    name: ceph-0
  - hostname: cephstorage-1
    name: ceph-1
  - hostname: cephstorage-2
    name: ceph-2
  defaults:
    profile: ceph-storage
    network_config:
      template: /home/stack/network/nic-configs/ceph-storage.j2
      network_config_update: true
    networks:
    - network: ctlplane
      vif: true
    - network: storage
    - network: storage_mgmt
    - network: storage_nfs
----
. Edit the network configuration template file, for example, `/home/stack/network/nic-configs/ceph-storage.j2`, for the {CephCluster} nodes
to include an interface that connects to the `StorageNFS` network:
+
[source,yaml]
----
- type: vlan
  device: nic2
  vlan_id: {{ storage_nfs_vlan_id }}
  addresses:
  - ip_netmask: {{ storage_nfs_ip }}/{{ storage_nfs_cidr }}
  routes: {{ storage_nfs_host_routes }}
----
. Update the {CephCluster} nodes:
+
----
$ openstack overcloud node provision \
    --stack overcloud   \
    --network-config -y  \
    -o overcloud-baremetal-deployed-storage_nfs.yaml \
    --concurrency 2 \
    /home/stack/network/baremetal_deployment.yaml
----
+
When the update is complete, ensure that the {CephCluster} nodes have a
new interface created and tagged with the VLAN that is associated with
`StorageNFS`.

. Identify an IP address from the `StorageNFS` network to use as the Virtual IP
address for the Ceph NFS service. You must provide this IP address in place of
the `{{ VIP }}` in the example below:
+
----
$ openstack port list -c "Fixed IP Addresses" --network storage_nfs
----
+
* Pick an appropriate size for the NFS cluster. The NFS service provides
active/active high availability when the cluster size is more than
one node. It is recommended that the ``{{ cluster_size }}`` is at least one
less than the number of hosts identified. This solution has been tested
with a 3-node NFS cluster.
* You must set the `ingress-mode` argument to `haproxy-protocol`. No other
ingress-mode is supported. This ingress mode allows you to enforce client
restrictions through the {rhos_component_storage_file}.
ifeval::["{build}" != "downstream"]
* For more information on deploying the clustered Ceph NFS service, see the
link:https://docs.ceph.com/en/latest/cephadm/services/nfs/[ceph orchestrator
documentation].
endif::[]
ifeval::["{build}" != "upstream"]
For more information on deploying the clustered Ceph NFS service, see the
link:https://access.redhat.com/documentation/en-us/red_hat_ceph_storage/7/html-single/operations_guide/index#management-of-nfs-ganesha-gateway-using-the-ceph-orchestrator[Management of NFS-Ganesha gateway using the Ceph Orchestrator (Limited Availability)] in _Red Hat Ceph Storage 7 Operations Guide_.
endif::[]
* The following commands are run inside a `cephadm shell` to create a clustered
Ceph NFS service.
+
----

# wait for shell to come up, then execute:
ceph orch host ls

# Identify the hosts that can host the NFS service.
# Repeat the following command to label each host identified:
ceph orch host label add <HOST> nfs

# Set the appropriate {{ cluster_size }} and {{ VIP }}:
ceph nfs cluster create cephfs \
    "{{ cluster_size }} label:nfs" \
    --ingress \
    --virtual-ip={{ VIP }}
    --ingress-mode=haproxy-protocol
}}

# Check the status of the nfs cluster with these commands
ceph nfs cluster ls
ceph nfs cluster info cephfs
----
