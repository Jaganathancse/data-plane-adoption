[id="configuring-target-nodes-for-ceph-monitor-migration_{context}"]

= Configuring target nodes for Ceph Monitor migration

Prepare the target {Ceph} nodes for the Ceph Monitor migration by performing the following actions:

. Enable firewall rules in a target node and persist them.
. Create a spec that is based on labels and apply it by using `cephadm`.
. Ensure that the Ceph Monitor quorum is maintained during the migration process.

.Procedure

. SSH into the target node and enable the firewall rules that are required to
reach a Ceph Monitor service:
+
----
$ for port in 3300 6789; {
    ssh heat-admin@<target_node> sudo iptables -I INPUT \
    -p tcp -m tcp --dport $port -m conntrack --ctstate NEW \
    -j ACCEPT;
}
----
+
* Replace `<target_node>` with the hostname of the node that hosts the new Ceph Monitor.

. Check that the rules are properly applied to the target node and persist them:
+
----
$ sudo iptables-save
$ sudo systemctl restart iptables
----

. To migrate the existing Ceph Monitors to the target {Ceph} nodes, create the following {Ceph} spec from the first Ceph Monitor, or the first Controller node, and add the `label:mon` section to the `placement` section:
+
[source,yaml]
----
service_type: mon
service_id: mon
placement:
  label: mon
----

. Save the spec in the `/tmp/mon.yaml` file.

. Apply the spec with `cephadm` by using the Ceph Orchestrator:
+
----
$ sudo cephadm shell -m /tmp/mon.yaml
$ ceph orch apply -i /mnt/mon.yaml
----

. Apply the `mon` label to the remaining {Ceph} target nodes to ensure that
quorum is maintained during the migration process:
+
----
declare -A target_nodes

target_nodes[mon]="oc0-ceph-0 oc0-ceph-1 oc0-ceph2"

mon_nodes="${target_nodes[mon]}"
IFS=' ' read -r -a mons <<< "$mon_nodes"

for node in "${mons[@]}"; do
    ceph orch host add label $node mon
    ceph orch host add label $node _admin
done
----
+
[NOTE]
Applying the `mon.yaml` spec allows the existing strategy to use `labels`
instead of `hosts`. As a result, any node with the `mon` label can host a Ceph
Monitor daemon. Perform this step only once to avoid multiple iterations when multiple Ceph Monitors are migrated.

. Check the status of the {CephCluster} and the Ceph Orchestrator daemons list.
Ensure that Ceph Monitors are in a quorum and listed by the `ceph orch` command:
+
----
# ceph -s
  cluster:
    id:     f6ec3ebe-26f7-56c8-985d-eb974e8e08e3
    health: HEALTH_OK

  services:
    mon: 6 daemons, quorum oc0-controller-0,oc0-controller-1,oc0-controller-2,oc0-ceph-0,oc0-ceph-1,oc0-ceph-2 (age 19m)
    mgr: oc0-controller-0.xzgtvo(active, since 32m), standbys: oc0-controller-1.mtxohd, oc0-controller-2.ahrgsk
    osd: 8 osds: 8 up (since 12m), 8 in (since 18m); 1 remapped pgs

  data:
    pools:   1 pools, 1 pgs
    objects: 0 objects, 0 B
    usage:   43 MiB used, 400 GiB / 400 GiB avail
    pgs:     1 active+clean
----
+
----
[ceph: root@oc0-controller-0 /]# ceph orch host ls
HOST              ADDR           LABELS          STATUS
oc0-ceph-0        192.168.24.14  osd mon _admin
oc0-ceph-1        192.168.24.7   osd mon _admin
oc0-ceph-2        192.168.24.8   osd mon _admin
oc0-controller-0  192.168.24.15  _admin mgr mon
oc0-controller-1  192.168.24.23  _admin mgr mon
oc0-controller-2  192.168.24.13  _admin mgr mon
----

.Next steps

Proceed to the next step xref:draining-the-source-node_{context}[Draining the source node].
